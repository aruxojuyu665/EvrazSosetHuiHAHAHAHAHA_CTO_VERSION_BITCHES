# Архитектура и логика работы RAG системы

## Обзор системы

RAG система для анализа документов ГОСТ представляет собой комплексное решение, объединяющее технологии векторного поиска, машинного обучения и генерации естественного языка для автоматического извлечения структурированной информации из технических документов.

## Архитектура системы

### Высокоуровневая архитектура

```
┌─────────────────────────────────────────────────────────────────┐
│                         USER INTERFACE                          │
│                     (CLI / Future: REST API)                    │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│                      APPLICATION LAYER                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │   Config     │  │  RAG System  │  │   Parsers    │         │
│  │   Manager    │  │   (LlamaIndex)│  │   (PDF)      │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│                      PROCESSING LAYER                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │  Embeddings  │  │  Chunking    │  │  Retrieval   │         │
│  │  (OpenAI)    │  │  (Sentence   │  │  (Vector     │         │
│  │              │  │   Splitter)  │  │   Search)    │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│                       STORAGE LAYER                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │    Milvus    │  │     etcd     │  │    MinIO     │         │
│  │  (Vectors)   │  │  (Metadata)  │  │  (Objects)   │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│                      GENERATION LAYER                           │
│  ┌──────────────────────────────────────────────────┐          │
│  │         Claude 3.5 Sonnet (via OpenRouter)       │          │
│  │              Response Generation                  │          │
│  └──────────────────────────────────────────────────┘          │
└─────────────────────────────────────────────────────────────────┘
```

## Компоненты системы

### 1. Конфигурационный слой (`src/config.py`)

**Назначение:** Централизованное управление настройками системы.

**Функции:**
- Загрузка переменных окружения из `.env` файла
- Валидация конфигурации при запуске
- Предоставление типизированного доступа к настройкам

**Основные параметры:**
- `OpenRouterConfig` - настройки LLM (модель, температура, токены)
- `EmbeddingConfig` - настройки embedding модели
- `MilvusConfig` - параметры подключения к Milvus
- `RAGConfig` - параметры RAG (chunk_size, overlap, top_k)
- `PathConfig` - пути к данным

### 2. Milvus Manager (`src/vector_store/milvus_store.py`)

**Назначение:** Управление векторной базой данных Milvus.

**Основные функции:**

#### 2.1. Подключение к Milvus
```python
def connect() -> bool
```
- Устанавливает соединение с Milvus сервером
- Использует параметры из конфигурации (host, port)
- Возвращает статус подключения

#### 2.2. Создание коллекции
```python
def create_collection(overwrite: bool = False) -> bool
```
- Создает коллекцию с заданной схемой
- Поля коллекции:
  - `id` (INT64, auto_id) - уникальный идентификатор
  - `embedding` (FLOAT_VECTOR, dim=1536) - векторное представление
  - `text` (VARCHAR) - текстовое содержимое
  - `metadata` (VARCHAR) - метаданные в JSON формате
- Создает IVF_FLAT индекс для векторного поиска
- Параметры индекса: metric_type="L2", nlist=1024

#### 2.3. Получение векторного хранилища
```python
def get_vector_store() -> MilvusVectorStore
```
- Возвращает объект MilvusVectorStore для интеграции с LlamaIndex
- Используется для создания индекса и выполнения поиска

### 3. RAG System (`src/rag/rag_system.py`)

**Назначение:** Основная логика RAG системы.

#### 3.1. Инициализация

```python
def __init__(
    openrouter_api_key: Optional[str] = None,
    embedding_api_key: Optional[str] = None,
    milvus_host: Optional[str] = None,
    milvus_port: Optional[int] = None
)
```

**Процесс инициализации:**
1. Загрузка конфигурации
2. Настройка LLM (Claude 3.5 Sonnet через OpenRouter)
3. Настройка embedding модели (OpenAI text-embedding-3-small)
4. Установка глобальных настроек LlamaIndex

#### 3.2. Настройка LLM

```python
def _setup_llm()
```

**Процесс:**
1. Создание клиента OpenAI с параметрами OpenRouter:
   - `api_key` - ключ OpenRouter
   - `api_base` - https://openrouter.ai/api/v1
   - `model` - anthropic/claude-3.5-sonnet
   - `temperature` - 0.1 (для детерминированности)
   - `max_tokens` - 4096

2. Установка в глобальные настройки LlamaIndex:
   ```python
   Settings.llm = self.llm
   ```

#### 3.3. Настройка Embeddings

```python
def _setup_embeddings()
```

**Процесс:**
1. Создание OpenAI embedding клиента:
   - `model` - text-embedding-3-small
   - `api_key` - ключ OpenAI
   - Размерность векторов: 1536

2. Установка в глобальные настройки:
   ```python
   Settings.embed_model = self.embed_model
   ```

## Рабочие процессы (Workflows)

### Workflow 1: Индексирование документов

```
┌─────────────┐
│  PDF File   │
└──────┬──────┘
       │
       ▼
┌─────────────────────────────────┐
│  1. Load Documents              │
│  SimpleDirectoryReader          │
│  - Reads PDF file(s)            │
│  - Extracts text content        │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│  2. Text Chunking               │
│  SentenceSplitter               │
│  - chunk_size: 1000 chars       │
│  - chunk_overlap: 200 chars     │
│  - Preserves sentence boundaries│
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│  3. Generate Embeddings         │
│  OpenAI Embedding Model         │
│  - Model: text-embedding-3-small│
│  - Dimension: 1536              │
│  - Batch processing             │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│  4. Store in Milvus             │
│  Vector Database                │
│  - Insert vectors               │
│  - Store text & metadata        │
│  - Create index (IVF_FLAT)      │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────┐
│  Index Ready│
└─────────────┘
```

**Детальное описание этапов:**

#### Этап 1: Загрузка документов
```python
def load_documents(document_path: str) -> List
```

**Процесс:**
1. Определение типа входа (файл или директория)
2. Использование `SimpleDirectoryReader` из LlamaIndex
3. Извлечение текста из PDF с сохранением структуры
4. Создание объектов `Document` с метаданными:
   - Имя файла
   - Путь к файлу
   - Дата создания
   - Размер файла

#### Этап 2: Разбиение на чанки
```python
text_splitter = SentenceSplitter(
    chunk_size=config.rag.chunk_size,      # 1000
    chunk_overlap=config.rag.chunk_overlap  # 200
)
```

**Логика разбиения:**
1. Текст разбивается на предложения
2. Предложения группируются в чанки размером ~1000 символов
3. Чанки перекрываются на 200 символов для сохранения контекста
4. Сохраняются границы предложений (не разрывает предложения)

**Пример:**
```
Chunk 1: [0:1000]    "Текст от начала до 1000 символов..."
Chunk 2: [800:1800]  "...перекрытие 200 символов + новый текст..."
Chunk 3: [1600:2600] "...перекрытие 200 символов + новый текст..."
```

#### Этап 3: Генерация embeddings
```python
embed_model = OpenAIEmbedding(
    api_key=self.embedding_api_key,
    model=config.embedding.model
)
```

**Процесс:**
1. Каждый чанк отправляется в OpenAI API
2. Модель `text-embedding-3-small` генерирует вектор размерности 1536
3. Векторы нормализуются (L2 normalization)
4. Batch обработка для оптимизации (до 100 чанков за раз)

#### Этап 4: Сохранение в Milvus
```python
def create_index(documents: List, show_progress: bool = True)
```

**Процесс:**
1. Создание `StorageContext` с Milvus vector store
2. Создание `VectorStoreIndex` из документов
3. Автоматическая вставка данных в Milvus:
   - Векторы embeddings
   - Текстовое содержимое чанков
   - Метаданные (источник, позиция в документе)
4. Создание индекса для быстрого поиска

### Workflow 2: Извлечение информации (Query)

```
┌─────────────┐
│   Query     │
│  (Question) │
└──────┬──────┘
       │
       ▼
┌─────────────────────────────────┐
│  1. Query Embedding             │
│  OpenAI Embedding Model         │
│  - Convert query to vector      │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│  2. Vector Search               │
│  Milvus Retrieval               │
│  - Similarity search (L2)       │
│  - Retrieve top_k chunks        │
│  - Score: similarity score      │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│  3. Context Assembly            │
│  Response Synthesizer           │
│  - Combine retrieved chunks     │
│  - Sort by relevance            │
│  - Prepare context for LLM      │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────────────────────────┐
│  4. Response Generation         │
│  Claude 3.5 Sonnet              │
│  - Read context                 │
│  - Generate answer              │
│  - Structure information        │
└──────┬──────────────────────────┘
       │
       ▼
┌─────────────┐
│  Response   │
│  + Sources  │
└─────────────┘
```

**Детальное описание этапов:**

#### Этап 1: Векторизация запроса
```python
query_embedding = embed_model.get_text_embedding(question)
```

**Процесс:**
1. Запрос пользователя преобразуется в вектор размерности 1536
2. Используется та же модель, что и для индексирования
3. Обеспечивается семантическая совместимость с индексом

#### Этап 2: Векторный поиск
```python
retriever = VectorIndexRetriever(
    index=self.index,
    similarity_top_k=top_k  # default: 5
)
nodes = retriever.retrieve(question)
```

**Процесс поиска:**
1. Вычисление L2 расстояния между query_embedding и всеми векторами в Milvus
2. Формула: `distance = sqrt(sum((a_i - b_i)^2))`
3. Сортировка по расстоянию (меньше = более релевантно)
4. Выбор top_k наиболее релевантных чанков
5. Каждый результат содержит:
   - Текст чанка
   - Score (similarity score)
   - Метаданные (источник, позиция)

**Пример результата:**
```python
[
    {
        "text": "Класс прочности C235 характеризуется...",
        "score": 0.92,
        "metadata": {"source": "GOST_27772-2021.pdf", "page": 5}
    },
    {
        "text": "Химический состав стали C235: углерод...",
        "score": 0.88,
        "metadata": {"source": "GOST_27772-2021.pdf", "page": 12}
    },
    ...
]
```

#### Этап 3: Сборка контекста
```python
response_synthesizer = get_response_synthesizer(
    response_mode="compact"
)
```

**Режимы сборки контекста:**

1. **compact** (используется по умолчанию):
   - Объединяет все релевантные чанки в один контекст
   - Удаляет дубликаты
   - Сортирует по релевантности
   - Оптимизирует размер для LLM

2. **tree_summarize**:
   - Иерархическая суммаризация
   - Используется для больших объемов данных

3. **refine**:
   - Итеративное улучшение ответа
   - Каждый чанк уточняет предыдущий ответ

**Формирование промпта:**
```
Context:
{retrieved_chunks}

Question:
{user_question}

Instructions:
Используя предоставленный контекст, ответь на вопрос.
Структурируй информацию и укажи конкретные значения.
```

#### Этап 4: Генерация ответа
```python
response = self.query_engine.query(question)
```

**Процесс генерации:**
1. Отправка промпта в Claude 3.5 Sonnet через OpenRouter API
2. Параметры генерации:
   - `temperature: 0.1` - для детерминированности и точности
   - `max_tokens: 4096` - достаточно для развернутых ответов
   - `top_p: 1.0` - без nucleus sampling

3. Claude анализирует контекст и генерирует структурированный ответ
4. Ответ включает:
   - Прямой ответ на вопрос
   - Конкретные значения и данные
   - Ссылки на источники (если есть в контексте)
   - Структурированное представление (списки, таблицы)

**Формат ответа:**
```python
{
    "answer": "Подробный ответ от Claude...",
    "source_nodes": [
        {
            "text": "Исходный текст чанка...",
            "score": 0.92,
            "metadata": {...}
        },
        ...
    ]
}
```

### Workflow 3: Извлечение информации о классе прочности

```python
def extract_strength_class_info(class_name: str = "C235") -> Dict
```

**Специализированный промпт:**
```
Извлеки всю доступную информацию о классе прочности стали {class_name} 
из документа ГОСТ 27772-2021.

Необходимо найти и структурировать следующую информацию:
1. Химический состав (массовая доля элементов в %)
2. Механические свойства (предел текучести, временное сопротивление, 
   относительное удлинение, ударная вязкость)
3. Предельные отклонения
4. Требования к испытаниям
5. Информация о сортаменте и видах продукции
6. Ссылки на другие стандарты, если они упоминаются в контексте {class_name}

Представь информацию в структурированном виде с указанием конкретных 
значений и диапазонов.
```

**Процесс:**
1. Выполняется обычный query workflow с специализированным промптом
2. Claude извлекает и структурирует информацию по заданным категориям
3. Результат сохраняется в JSON формате

## Оптимизации и особенности

### 1. Chunking Strategy

**Почему важен chunk_size:**
- Слишком маленький (< 500): теряется контекст
- Слишком большой (> 2000): снижается точность поиска
- Оптимальный (1000): баланс между контекстом и точностью

**Почему важен overlap:**
- Предотвращает потерю информации на границах чанков
- 200 символов (~1-2 предложения) обеспечивают непрерывность

### 2. Vector Search Optimization

**IVF_FLAT индекс:**
- Inverted File with Flat compression
- Быстрый поиск для средних объемов данных (< 1M векторов)
- Параметр `nlist=1024`: количество кластеров для индексации

**L2 метрика:**
- Euclidean distance
- Хорошо работает для normalized embeddings
- Альтернатива: IP (Inner Product) или COSINE

### 3. LLM Configuration

**Temperature = 0.1:**
- Низкая температура для детерминированности
- Минимизирует "творческие" ответы
- Фокус на точности и фактах

**Max tokens = 4096:**
- Достаточно для развернутых ответов
- Позволяет включать таблицы и списки
- Баланс между детальностью и стоимостью

## Обработка ошибок

### 1. Подключение к Milvus
```python
try:
    connections.connect(...)
except Exception as e:
    logger.error(f"Ошибка подключения к Milvus: {e}")
    return False
```

### 2. API вызовы
- Автоматические retry для временных ошибок
- Логирование всех ошибок
- Graceful degradation

### 3. Валидация данных
- Проверка конфигурации при запуске
- Валидация входных параметров
- Проверка существования файлов

## Масштабируемость

### Текущие ограничения:
- Milvus standalone: до 1M векторов
- Одновременные запросы: ограничены API rate limits
- Память: зависит от размера индекса

### Возможности масштабирования:
1. **Horizontal scaling**: несколько инстансов приложения
2. **Milvus cluster**: для больших объемов данных
3. **Caching**: Redis для кэширования частых запросов
4. **Async processing**: для batch операций

## Мониторинг и логирование

### Логирование:
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### Метрики:
- Количество индексированных документов
- Время выполнения запросов
- Количество векторов в Milvus
- API usage (tokens, requests)

## Безопасность

### API ключи:
- Хранятся в переменных окружения
- Не коммитятся в Git
- Валидируются при запуске

### Данные:
- Не содержат конфиденциальной информации (публичные ГОСТы)
- Можно использовать внешние API (OpenRouter, OpenAI)

## Заключение

Система представляет собой полноценное RAG решение, оптимизированное для работы с техническими документами ГОСТ. Архитектура обеспечивает баланс между точностью, производительностью и масштабируемостью, позволяя эффективно извлекать структурированную информацию из неструктурированных PDF документов.
